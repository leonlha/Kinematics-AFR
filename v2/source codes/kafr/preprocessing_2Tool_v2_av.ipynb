{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter left hand tool with ID = 0 (which appears most)\n",
    "#\n",
    "#\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the file path\n",
    "\n",
    "folder_path = r'D:\\Research\\Phong\\ToolTracking_Project\\centroid'\n",
    "\n",
    "# List all files in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "# print(file_names)\n",
    "for file_name in tqdm(file_names, desc='Processing csv files'):\n",
    "    if file_name.endswith('.csv'):\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(folder_path, file_name))\n",
    "\n",
    "        # Assuming 'entroid' contains tuples as strings, convert them to actual tuples\n",
    "        # df['centroid'] = df['centroid'].apply(lambda x: tuple(map(float, x.strip('()').split(','))))\n",
    "        df['centroid'] = df['centroid'].apply(lambda x: tuple(round(float(coord), 4) for coord in x.strip('()').split(',')))\n",
    "\n",
    "        # Filter based on 'Class ID'\n",
    "        classID_df = df[df['Class ID'] == 0]\n",
    "\n",
    "        # Get the right hand tool\n",
    "        filtered_df = classID_df[classID_df['centroid'].apply(lambda x: x[0] <= 0.5)]\n",
    "\n",
    "        # Remove duplicate rows based on 'frame_number'\n",
    "        cleaned_df = filtered_df.drop_duplicates(subset='frame_number', keep='first')\n",
    "\n",
    "        # Define the folder path for cleaned data\n",
    "        cleaned_folder = '1_cleaned_df'\n",
    "        os.makedirs(cleaned_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "        # Extract the identifier from the filename\n",
    "        file_identifier = file_name.split('_')[-1]\n",
    "\n",
    "        # Define the output file path for cleaned data\n",
    "        cleaned_file_path = os.path.join(cleaned_folder, f'1_cleaned_df_{file_identifier}')\n",
    "\n",
    "        # Save the filtered DataFrame to a new file\n",
    "        cleaned_df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "        \n",
    "print(f\"Cleaned data saved to '{cleaned_folder}' folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing csv files: 100%|██████████| 42/42 [01:18<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled data saved to corrected_data folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the file path\n",
    "\n",
    "folder_path = r'1_cleaned_df'\n",
    "\n",
    "# List all files in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "# print(file_names)\n",
    "for file_name in tqdm(file_names, desc='Processing csv files'):\n",
    "    if file_name.endswith('.csv'):\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(folder_path, file_name))\n",
    "\n",
    "        # Sort DataFrame by frame_number\n",
    "        df = df.sort_values(by='frame_number')\n",
    "\n",
    "        # Initialize variables\n",
    "        previous_xy = None\n",
    "        filled_rows = []\n",
    "\n",
    "        # Iterate through each row in the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            # If the frame_number is greater than the previous frame_number + 1, fill missing frames\n",
    "            if previous_xy is not None and row['frame_number'] != previous_frame_number + 1:\n",
    "                for i in range(previous_frame_number + 1, row['frame_number']):\n",
    "                    # Create a new row with the frame_number and xy_cords from the previous frame\n",
    "                    new_row = (i, previous_class_id, previous_xy)\n",
    "                    filled_rows.append(new_row)\n",
    "            \n",
    "            # Update previous_xy and previous_frame_number for the next iteration\n",
    "            previous_xy = row['centroid']\n",
    "            previous_frame_number = row['frame_number']\n",
    "            previous_class_id = row['Class ID']\n",
    "            \n",
    "            # Add the current row to filled_rows\n",
    "            filled_rows.append((row['frame_number'], row['Class ID'], row['centroid']))\n",
    "\n",
    "        # Fill xy cords for frame number 1 if missing\n",
    "        if filled_rows[0][0] != 1:\n",
    "            filled_rows.insert(0, (1, 0, (0, 0)))\n",
    "\n",
    "        # Create a DataFrame from filled_rows\n",
    "        filled_df = pd.DataFrame(filled_rows, columns=['frame_number', 'Class ID', 'centroid'])\n",
    "\n",
    "        # Sort DataFrame by frame_number\n",
    "        filled_df = filled_df.sort_values(by='frame_number')\n",
    "\n",
    "        # Define the folder path for cleaned data\n",
    "        corrected_data_folder = '2_corrected_data'\n",
    "        os.makedirs(corrected_data_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "        # Extract the identifier from the filename\n",
    "        file_identifier = file_name.split('_')[-1]\n",
    "\n",
    "        # Define the output file path\n",
    "        corrected_data_path = os.path.join(corrected_data_folder, f'2_corrected_data_{file_identifier}')\n",
    "\n",
    "\n",
    "        # Save filled DataFrame to a new file\n",
    "        filled_df.to_csv(corrected_data_path, index=False)\n",
    "\n",
    "print(\"Filled data saved to corrected_data folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Open the file to read data\n",
    "# with open('4_condition_met_indexes.csv', 'r') as file:\n",
    "#     # Read the CSV file\n",
    "#     df = pd.read_csv(file)\n",
    "\n",
    "\n",
    "\n",
    "# # Print the first few rows of the DataFrame to verify data reading\n",
    "# print(df.head())\n",
    "\n",
    "# # Calculate differences between indices\n",
    "# index_diff = [df['frame_number'][i+1] - df['frame_number'][i] for i in range(len(df['frame_number'])-1)]\n",
    "\n",
    "# # Plotting the differences between indices\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.plot(index_diff, marker='o', linestyle='-')\n",
    "# plt.xlabel('Data Point')\n",
    "# plt.ylabel('Frame Difference')\n",
    "# plt.title('Difference Between Frame Changes')\n",
    "# plt.grid(True)\n",
    "# # Set y-axis limit\n",
    "# plt.ylim(0, 200)\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing csv files: 100%|██████████| 42/42 [00:00<00:00, 86.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved sorted 3_sorted folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the file path\n",
    "file_path = r'train.txt'  # Replace 'file.txt' with the path to your file\n",
    "folder_path = r'2_corrected_data'\n",
    "# Define the folder path for cleaned data\n",
    "sorted_folder = '3_sorted'\n",
    "\n",
    "# List all files in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "def sort_train(keyword):\n",
    "    # Read lines from the file and filter for lines containing the search keyword\n",
    "    filtered_lines = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if keyword in line:\n",
    "                filtered_lines.append(line.strip())\n",
    "\n",
    "        # Check if existing search_keyword in train file\n",
    "        if(len(filtered_lines) > 0):\n",
    "            # Create a DataFrame from filtered lines\n",
    "            df = pd.DataFrame([line.split('---') for line in filtered_lines],\n",
    "                            columns=['FileName', 'Class', 'NumberOfFrames', 'FrameRate'])\n",
    "\n",
    "            # Sort the DataFrame by the first column ('FileName')\n",
    "            df_sorted = df.sort_values(by='FileName')\n",
    "\n",
    "            os.makedirs(sorted_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "            # Define the output file path + .csv\n",
    "            sorted_path = os.path.join(sorted_folder, f'3_sorted_{search_keyword}.csv')\n",
    "\n",
    "            # Write the sorted DataFrame to a new CSV file\n",
    "            df_sorted.to_csv(sorted_path, index=False)\n",
    "\n",
    "# print(file_names)\n",
    "for file_name in tqdm(file_names, desc='Processing csv files'):\n",
    "    if file_name.endswith('.csv'):\n",
    "        # Extract the identifier from the filename and define a search keyword\n",
    "        search_keyword = os.path.splitext(file_name)[0].split('_')[-1]\n",
    "\n",
    "        sort_train(search_keyword)\n",
    "\n",
    "        \n",
    "print(f\"Processed and saved sorted {sorted_folder} folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing csv files: 100%|██████████| 30/30 [00:10<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching information based on frame numbers has been written to '4_merged' folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the folder path for cleaned data\n",
    "sorted_folder = r'3_sorted'\n",
    "corrected_data_path = r'2_corrected_data'\n",
    "\n",
    "merged_folder = r'4_merged'\n",
    "\n",
    "# Function to extract frame number from filename\n",
    "def extract_frame_number(filename):\n",
    "    parts = filename.split('Frame')\n",
    "    if len(parts) > 1:\n",
    "        frame_number = parts[1].split('.')[0]\n",
    "        return int(frame_number)\n",
    "    return None\n",
    "\n",
    "# List all files in the folder\n",
    "file_names = os.listdir(sorted_folder)\n",
    "\n",
    "# print(file_names)\n",
    "for file_name in tqdm(file_names, desc='Processing csv files'):\n",
    "    if file_name.endswith('.csv'):\n",
    "        \n",
    "        # Read the first CSV file into DataFrame\n",
    "        df_first = pd.read_csv(os.path.join(sorted_folder,file_name))\n",
    "\n",
    "        # Extract frame numbers from the 'FileName' column in df_first\n",
    "        df_first['frame_number'] = df_first['FileName'].apply(lambda x: extract_frame_number(x))\n",
    "\n",
    "        # Extract the identifier from the filename \n",
    "        video_id = os.path.splitext(file_name)[0].split('_')[-1]\n",
    "\n",
    "        # Read the second CSV file into DataFrame\n",
    "        second_file = os.path.join(corrected_data_path,f'2_corrected_data_{video_id}.csv')\n",
    "\n",
    "        df_second = pd.read_csv(second_file)\n",
    "\n",
    "        # Initialize a list to store the matched rows from df_second\n",
    "        matched_rows = []\n",
    "\n",
    "        # print(df_first)\n",
    "\n",
    "        # Loop through each frame number in df_first\n",
    "        for frame_number in df_first['frame_number']:\n",
    "            # Find all rows in df_second that match the current frame number\n",
    "            matching_rows = df_second[df_second['frame_number'] == frame_number]\n",
    "            # Append the matching rows to the list\n",
    "            matched_rows.extend(matching_rows.values.tolist())\n",
    "\n",
    "        # Create a DataFrame from the matched rows with column names from df_second\n",
    "        columns = df_second.columns\n",
    "        result_df = pd.DataFrame(matched_rows, columns=columns)\n",
    "\n",
    "        os.makedirs(merged_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "        # Define the output file path + .csv\n",
    "        merged_path = os.path.join(merged_folder, f'4_merged_{video_id}.csv')\n",
    "        \n",
    "        # Write the result DataFrame to the output CSV file\n",
    "        result_df.to_csv(merged_path, index=False)\n",
    "\n",
    "print(f\"Matching information based on frame numbers has been written to '{merged_folder}' folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing csv files: 100%|██████████| 30/30 [00:03<00:00,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to '{measured_folder}' folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define the folder path for cleaned data\n",
    "merged_folder = r'4_merged'\n",
    "measured_folder = r'5_measured'\n",
    "\n",
    "# List all files in the folder\n",
    "file_names = os.listdir(merged_folder)\n",
    "\n",
    "# print(file_names)\n",
    "for file_name in tqdm(file_names, desc='Processing csv files'):\n",
    "    if file_name.endswith('.csv'):\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(merged_folder,file_name))\n",
    "\n",
    "        # Convert xy cords to numerical values\n",
    "        df['centroid'] = df['centroid'].apply(lambda x: np.array(eval(x)))\n",
    "\n",
    "        # Function to calculate Euclidean distance between two points\n",
    "        def euclidean_distance(point1, point2):\n",
    "            return np.linalg.norm(point2 - point1)\n",
    "\n",
    "        # Initialize variables\n",
    "        total_distances = []  # List to store distances for each interval\n",
    "        prev_xy = None\n",
    "        current_interval_distance = 0\n",
    "\n",
    "        # Iterate through each row in the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "\n",
    "            # Append the total distance for the previous interval to the list\n",
    "            total_distances.append(current_interval_distance)\n",
    "            current_interval_distance = 0  # Reset the distance for the new interval\n",
    "            # prev_xy = None\n",
    "            \n",
    "            # Check if Class ID_x is 0\n",
    "            if row['Class ID'] == 0:\n",
    "                # Skip the first frame\n",
    "                if prev_xy is None:\n",
    "                    prev_xy = row['centroid']\n",
    "                    continue\n",
    "                \n",
    "                # Calculate distance traveled\n",
    "                distance = euclidean_distance(prev_xy, row['centroid'])\n",
    "                \n",
    "                # Update previous xy coordinates\n",
    "                prev_xy = row['centroid']\n",
    "                \n",
    "                # Add distance to the current interval's total\n",
    "                current_interval_distance += distance\n",
    "\n",
    "                # print(current_interval_distance)\n",
    "\n",
    "        # Append the last interval's total distance to the list\n",
    "        total_distances.append(current_interval_distance)\n",
    "\n",
    "        # Calculate velocity for each interval\n",
    "        # Assuming the time interval between frames is constant\n",
    "        time_interval = 1  # You need to specify the time interval between frames\n",
    "        velocities = np.diff(total_distances) / time_interval\n",
    "\n",
    "        # Create DataFrame for results with frame_number, Interval, Distance, and Velocity\n",
    "        result_df = pd.DataFrame({\n",
    "            'frame_number': df['frame_number'],  # Use frame_number from the original DataFrame\n",
    "            'Distance': total_distances[:-1],  # Exclude the last interval's distance\n",
    "            'Velocity': velocities\n",
    "        })\n",
    "\n",
    "        # Extract the identifier from the filename \n",
    "        video_id = os.path.splitext(file_name)[0].split('_')[-1]\n",
    "\n",
    "        os.makedirs(measured_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "        # Define the output file path + .csv\n",
    "        measured_path = os.path.join(measured_folder, f'5_measured_{video_id}.csv')\n",
    "\n",
    "        # Save the result to a CSV file\n",
    "        result_df.to_csv(measured_path, index=False)\n",
    "\n",
    "print(\"Results saved to '{measured_folder}' folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing csv files: 100%|██████████| 30/30 [00:00<00:00, 72.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_and_sort_csv(source_folder, target_folder, output_folder):\n",
    "        \n",
    "    # Get list of CSV files in source folder\n",
    "    source_files = [f for f in os.listdir(source_folder) if f.endswith('.csv')]\n",
    "    \n",
    "    # for filename in source_files:\n",
    "    for filename in tqdm(source_files, desc='Processing csv files'):\n",
    "        # Construct paths for source and target files\n",
    "        source_file_path = os.path.join(source_folder, filename)\n",
    "        target_file_path = os.path.join(target_folder, filename)\n",
    "        \n",
    "        # Read CSV files into DataFrames\n",
    "        source_df = pd.read_csv(source_file_path)\n",
    "        \n",
    "        target_df = pd.read_csv(target_file_path)\n",
    "        \n",
    "        # Concatenate both DataFrames\n",
    "        combined_df = pd.concat([source_df, target_df], ignore_index=True)\n",
    "        \n",
    "        # Sort DataFrame by 'frame_number'\n",
    "        combined_df.sort_values(by='frame_number', inplace=True)\n",
    "\n",
    "        # Extract the identifier from the filename \n",
    "        video_id = os.path.splitext(filename)[0].split('_')[-1]\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "        # Define the output file path + .csv\n",
    "        combined_path = os.path.join(output_folder, f'6_combined_{video_id}.csv')\n",
    "\n",
    "        # Write sorted DataFrame to a new CSV file\n",
    "        combined_df.to_csv(combined_path, index=False)\n",
    "            \n",
    "# Specify paths to source folder (5_measured), target folder, and output folder\n",
    "source_folder_path = r'5_measured'\n",
    "target_folder_path = r'data1\\5_measured'\n",
    "output_folder_path = r'6_mixed'\n",
    "\n",
    "# Call the function to process CSV files in the source folder\n",
    "merge_and_sort_csv(source_folder_path, target_folder_path, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use velocity and distance instead of distance only\n",
    "import os\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import math\n",
    "\n",
    "# Define the folder path for cleaned data\n",
    "measured_folder = r'6_mixed'\n",
    "threshold_folder = r'7_threshold'\n",
    "\n",
    "# Define percentage of samples\n",
    "number_of_tool = 2\n",
    "percent_sample = 0.15/number_of_tool\n",
    "print(percent_sample)\n",
    "\n",
    "def process_and_write(file_name):\n",
    "    if file_name.endswith('.csv'):\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(measured_folder,file_name))\n",
    "\n",
    "        # Assuming 'Velocity' column contains the velocity\n",
    "        distances = df['Distance']\n",
    "        velocities = df['Velocity']\n",
    "\n",
    "        print('filename:',file_name,' number of samples:', len(distances))\n",
    "\n",
    "        # Specify the desired number of indices\n",
    "        desired_indices = math.ceil(percent_sample * len(distances))\n",
    "\n",
    "        # Filter rows where abs(velocity) > 0\n",
    "        # filtered_df = df[abs(df['Velocity']) > 0]\n",
    "        # use all data\n",
    "        filtered_df = df\n",
    "\n",
    "        # Count the number of rows where distance > 0\n",
    "        count_greater_than_zero = filtered_df.shape[0]\n",
    "\n",
    "        print(count_greater_than_zero < desired_indices)\n",
    "\n",
    "        if count_greater_than_zero < desired_indices:\n",
    "            # Initialize a list to store rows (converted to tuples)\n",
    "            indices_where_condition_met = []\n",
    "\n",
    "            # Iterate over rows in filtered_df and append each row to indices_where_condition_met\n",
    "            for index, row in filtered_df.iterrows():\n",
    "                indices_where_condition_met.append(index)\n",
    "        else:\n",
    "            # Initialize threshold value\n",
    "            threshold_value = 0.01#1  # \n",
    "\n",
    "            # Initialize variables\n",
    "            indices_where_condition_met = []  # List to store indices where condition is met\n",
    "            index = 0\n",
    "\n",
    "            # Iterate until the desired number of indices is reached\n",
    "            while len(indices_where_condition_met) < desired_indices:\n",
    "                # Reset variables\n",
    "                indices_where_condition_met = []\n",
    "                index = 0\n",
    "                \n",
    "                # Loop until all indices are read\n",
    "                while index < len(distances):\n",
    "                    # Initialize current sum for the current set of intervals\n",
    "                    current_sum = 0  + 1e-6\n",
    "                    \n",
    "                    # Loop over intervals starting from the current index\n",
    "                    for i in range(index, len(distances)):\n",
    "                        # Add the distance to the current sum\n",
    "                        current_sum = current_sum + distances[i] + abs(velocities[i])  # Adding 1e-6 to each distance\n",
    "                        \n",
    "                        # Check if the inverse of the sum exceeds the threshold\n",
    "                        if (1 / current_sum) <= threshold_value:\n",
    "                            # Add the index where the condition is met to the list\n",
    "                            indices_where_condition_met.append(i) \n",
    "                            \n",
    "                            # Move to the next index after this interval\n",
    "                            index = i + 1\n",
    "                            break\n",
    "                    else:\n",
    "                        # If no break occurred, all distances were considered and index should be set to the length of distances\n",
    "                        index = len(distances)\n",
    "                    \n",
    "                    # print((1 / current_sum))\n",
    "                    # print('len',len(indices_where_condition_met))\n",
    "                print('number of indices:',len(indices_where_condition_met), '/ desired len:',desired_indices)\n",
    "                # Adjust the threshold value for the next iteration\n",
    "                threshold_value += 0.01  # Increase the threshold by a small amount\n",
    "\n",
    "            print(\"Threshold value:\", threshold_value)\n",
    "            print(\"Number of indices meeting the condition:\", len(indices_where_condition_met))\n",
    "\n",
    "        # Extract the identifier from the filename \n",
    "        video_id = os.path.splitext(file_name)[0].split('_')[-1]\n",
    "\n",
    "        os.makedirs(threshold_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "        # Define the output file path + .csv\n",
    "        threshold_path = os.path.join(threshold_folder, f'7_threshold_{video_id}.csv')\n",
    "\n",
    "        # Save lines corresponding to the indices where the condition is met\n",
    "        with open(threshold_path, \"w\") as output_file:\n",
    "            # Write the header line\n",
    "            output_file.write('frame_number\\n')\n",
    "            \n",
    "            # Write the indices to the file\n",
    "            for idx in indices_where_condition_met:\n",
    "                # Write the index to the output file\n",
    "                output_file.write(str(df['frame_number'].iloc[idx]) + '\\n')\n",
    "    \n",
    "    # return  threshold_value           \n",
    "\n",
    "# List all files in the folder\n",
    "file_names = os.listdir(measured_folder)\n",
    "\n",
    "# Specify the number of parallel processes\n",
    "num_cores = -1  # Use all available CPU cores\n",
    "\n",
    "# Execute the tasks in parallel\n",
    "Parallel(n_jobs=num_cores)(\n",
    "    delayed(process_and_write)(file_name)\n",
    "    for file_name in file_names\n",
    "    )\n",
    "# for file_name in file_names:\n",
    "#     (process_and_write)(file_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing csv files: 100%|██████████| 30/30 [00:00<00:00, 46.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame based on matching frame numbers has been written to '8_filtered' in text format with custom formatting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the folder path for cleaned data\n",
    "sorted_folder = r'3_sorted'\n",
    "threshold_folder = r'7_threshold'\n",
    "filtered_folder = r'8_filtered'\n",
    "\n",
    "# Function to extract frame number from filename\n",
    "def extract_frame_number(filename):\n",
    "    parts = filename.split('Frame')\n",
    "    if len(parts) > 1:\n",
    "        frame_number = parts[1].split('.')[0]\n",
    "        return int(frame_number)\n",
    "    return None\n",
    "\n",
    "# List all files in the folder\n",
    "file_names = os.listdir(sorted_folder)\n",
    "\n",
    "# print(file_names)\n",
    "for file_name in tqdm(file_names, desc='Processing csv files'):\n",
    "    if file_name.endswith('.csv'):\n",
    "\n",
    "        # Read the first CSV file into DataFrame\n",
    "        first_file = os.path.join(sorted_folder,file_name)\n",
    "        df_first = pd.read_csv(first_file)\n",
    "\n",
    "        # Extract frame numbers from the 'FileName' column in df_first\n",
    "        df_first['frame_number'] = df_first['FileName'].apply(lambda x: extract_frame_number(x))\n",
    "\n",
    "        # Extract the identifier from the filename \n",
    "        video_id = os.path.splitext(file_name)[0].split('_')[-1]\n",
    "        \n",
    "        # Read the frame numbers from the second CSV file into a list\n",
    "        second_file = os.path.join(threshold_folder, f'7_threshold_{video_id}.csv')\n",
    "        df_second = pd.read_csv(second_file)\n",
    "\n",
    "        # Convert the 'frame_number' column in df_second to a list\n",
    "        frame_numbers_to_keep = df_second['frame_number'].tolist()\n",
    "\n",
    "        # Filter df_first to retain rows with frame numbers that match the ones in the list\n",
    "        df_filtered = df_first[df_first['frame_number'].isin(frame_numbers_to_keep)]\n",
    "\n",
    "        # Keep only unique filenames in the filtered DataFrame\n",
    "        df_filtered = df_filtered.drop_duplicates(subset='FileName')\n",
    "\n",
    "        # Drop the 'frame_number' column from df_filtered\n",
    "        df_filtered.drop('frame_number', axis=1, inplace=True)\n",
    "\n",
    "        os.makedirs(filtered_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "        # Define the output file path + .csv\n",
    "        filtered_path = os.path.join(filtered_folder, f'8_filtered_{video_id}.csv')\n",
    "\n",
    "        # Write the filtered DataFrame to a new text file with custom formatting\n",
    "        with open(filtered_path, 'w') as f:\n",
    "            # Write the DataFrame to the text file row by row with custom formatting\n",
    "            for _, row in df_filtered.iterrows():\n",
    "                # Convert row values to a list of strings\n",
    "                row_values = [str(value) for value in row.tolist()]\n",
    "                # Join row values with '---' and write to file\n",
    "                f.write('---'.join(row_values) + '\\n')\n",
    "\n",
    "print(f\"Filtered DataFrame based on matching frame numbers has been written to '{filtered_folder}' in text format with custom formatting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add \n",
    "# import os\n",
    "# import random\n",
    "\n",
    "# # Function to extract video name from CSV file name\n",
    "# def extract_video_name(csv_file):\n",
    "#     parts = csv_file.split('_')\n",
    "#     video_name = parts[-1].replace('.csv', '')\n",
    "#     return video_name\n",
    "\n",
    "# # Define percentage of samples\n",
    "# percent_sample = 0.15\n",
    "\n",
    "# # Define pattern\n",
    "# class_pattern = '---0---'\n",
    "\n",
    "# # Specify the folder containing CSV files\n",
    "# folder_path = r'8_filtered'\n",
    "\n",
    "# # List all CSV files in the folder\n",
    "# csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# # Specify the train.txt file path\n",
    "# train_file_path = r'train.txt'\n",
    "\n",
    "# # # Process each CSV file in the folder\n",
    "# # csv_file = r'7_filtered_GJ01182018MP.csv'\n",
    "\n",
    "# # if(1==1):\n",
    "# for csv_file in csv_files:\n",
    "#     # Get the full path of the CSV file\n",
    "#     csv_file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "#     # Get the video name from the CSV file name\n",
    "#     video_name = extract_video_name(csv_file)\n",
    "    \n",
    "#     # List to store lines with matching video name and class 0\n",
    "#     matching_lines = []\n",
    "    \n",
    "#     # Read the train.txt file and search for lines\n",
    "#     with open(train_file_path, 'r') as train_file:\n",
    "#         for line in train_file:\n",
    "#             if video_name in line and class_pattern in line:\n",
    "#                 matching_lines.append(line.strip())         \n",
    "    \n",
    "#     if(len(matching_lines) > 0):\n",
    "#         # Calculate the number of lines to select (5% of matching lines)\n",
    "#         num_lines_to_select = max(1, int(percent_sample * len(matching_lines)))\n",
    "        \n",
    "#         # Randomly select 5% of the matching lines\n",
    "#         random_selected_lines = random.sample(matching_lines, num_lines_to_select)\n",
    "        \n",
    "#         # Append the randomly selected lines to the target CSV file\n",
    "#         with open(csv_file_path, 'a') as target_file:\n",
    "#             for line in random_selected_lines:\n",
    "#                 target_file.write(line + '\\n')\n",
    "        \n",
    "#         print(f\"Successfully appended {num_lines_to_select} random lines to {csv_file_path}.\")\n",
    "#     else:\n",
    "#         print(f\"Skip {csv_file_path}.\")\n",
    "\n",
    "# print(\"Process completed for all CSV files in the folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled lines from CSV files written to: data2Tool_av_0_15_v1.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Define the folder path containing CSV files\n",
    "folder_path = r'8_filtered'\n",
    "\n",
    "# Initialize an empty list to store all lines from CSV files\n",
    "all_lines = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Check if the file is a CSV file\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read all lines from the CSV file\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        # Append all lines to the list of all lines\n",
    "        all_lines.extend(lines)\n",
    "\n",
    "# Shuffle all lines randomly\n",
    "random.shuffle(all_lines)\n",
    "\n",
    "# Define the output file path for the shuffled lines\n",
    "output_file_path = r'data2Tool_av_0_15_v1.txt'\n",
    "\n",
    "# Write the shuffled lines to the output file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    output_file.writelines(all_lines)\n",
    "\n",
    "print(f\"Shuffled lines from CSV files written to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data has been written to train_baseline_0_2_v1.txt.\n"
     ]
    }
   ],
   "source": [
    "# # Extract at interval\n",
    "# import pandas as pd\n",
    "\n",
    "# def extract_data_at_interval(file_path, interval_percentage):\n",
    "#     # Read the train.txt file and count total lines\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "#         total_lines = len(lines)\n",
    "    \n",
    "#     # Calculate number of lines to extract (5% of total lines)\n",
    "#     lines_to_extract = int(total_lines * interval_percentage)\n",
    "    \n",
    "#     # Determine the fixed interval for data extraction\n",
    "#     interval = total_lines // lines_to_extract\n",
    "    \n",
    "#     # Initialize a list to store extracted data\n",
    "#     extracted_data = []\n",
    "\n",
    "#     # Iterate through lines and extract data at the fixed interval\n",
    "#     for i in range(0, total_lines, interval):\n",
    "#         line = lines[i].strip()\n",
    "#         parts = line.split('---')\n",
    "#         filename = parts[0]\n",
    "#         label = int(parts[1])\n",
    "#         value1 = int(parts[2])\n",
    "#         value2 = int(parts[3])\n",
    "#         extracted_data.append((filename, label, value1, value2))\n",
    "    \n",
    "#     return extracted_data\n",
    "\n",
    "# def write_extracted_data_to_file(extracted_data, output_file_path):\n",
    "#     # Write extracted data to the specified output file\n",
    "#     with open(output_file_path, 'w') as output_file:\n",
    "#         for data in extracted_data:\n",
    "#             line = '---'.join([str(elem) for elem in data]) + '\\n'\n",
    "#             output_file.write(line)\n",
    "\n",
    "# # Example usage:\n",
    "# file_path = 'train.txt'\n",
    "# interval_percentage = 0.2  # % of the data\n",
    "# output_file_path = 'train_baseline_0_2_v1.txt'  # Path to the output file\n",
    "\n",
    "# # Extract data from train.txt\n",
    "# extracted_data = extract_data_at_interval(file_path, interval_percentage)\n",
    "\n",
    "# # Write extracted data to the output file\n",
    "# write_extracted_data_to_file(extracted_data, output_file_path)\n",
    "\n",
    "# print(f\"Extracted data has been written to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts:\n",
      "Class 0: 734 samples\n",
      "Class 1: 334 samples\n",
      "Class 2: 1245 samples\n",
      "Class 3: 622 samples\n",
      "Class 4: 1742 samples\n",
      "Class 5: 1422 samples\n",
      "Class 6: 997 samples\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the text file\n",
    "# file_path = 'train_baseline.txt'\n",
    "file_path = 'data2Tool_av_0_15_v1.txt'\n",
    "# file_path = 'train.txt'\n",
    "# file_path = 'train_baseline_0_3_v2.txt'\n",
    "\n",
    "# Dictionary to count occurrences of each class\n",
    "class_counts = {}\n",
    "\n",
    "# Read the file and analyze each line\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split each line by '---'\n",
    "        parts = line.strip().split('---')\n",
    "        \n",
    "        if len(parts) >= 2:\n",
    "            # Extract the class number (the second part after splitting)\n",
    "            class_number = parts[1].strip()\n",
    "            \n",
    "            # Update the class count in the dictionary\n",
    "            if class_number in class_counts:\n",
    "                class_counts[class_number] += 1\n",
    "            else:\n",
    "                class_counts[class_number] = 1\n",
    "\n",
    "# Print the class counts\n",
    "print(\"Class Counts:\")\n",
    "for class_num, count in sorted(class_counts.items()):\n",
    "    print(f\"Class {class_num}: {count} samples\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_ana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
